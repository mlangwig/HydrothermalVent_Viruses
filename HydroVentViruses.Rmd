---
title: "Hydrothermal Vent Viruses Protocol"
author: "Maggie Langwig"
date: "8/21/2022"
output:
  html_document: rmdformats::downcute
  toc: yes
  toc_float: yes
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Organizing the 43 assemblies

## Renaming
1. 11 assemblies do not have the sample name associated with the file so I am adding it.
```{bash, eval = FALSE}
cd /storage1/data12/Projects/Vent_Viruses/Assemblies_Renamed

for i in *.fasta; do awk '/>/{sub(">","&"FILENAME"_");sub(/\.fasta/,x)}1' $i >> $i.renamed; done
```

2. Special naming for S009:
```{bash, eval = FALSE}
for i in *.fasta; do awk '/>/{sub(">","&"FILENAME"_");sub(/\_metaspades_scaffolds.min1000.fasta/,x)}1' $i >> $i.renamed; done
```

3. Adding site identifier to assembly names (after checking your sed first!)

### Brothers UC
```{bash, eval = FALSE}
sed -i 's/>/>Brothers_UC_/g' S010_metaspades_scaffolds.min1000.fasta
```

```{bash, eval = FALSE}
sed -i 's/>/>Brothers_UC_/g' S011_metaspades_scaffolds.min1000.fasta
```

```{bash, eval = FALSE}
sed -i 's/>/>Brothers_UC_/g' S147_metaspades_scaffolds.min1000.fasta
```
### Brothers LC
```{bash, eval = FALSE}
sed -i 's/>/>Brothers_LC_/g' S014_metaspades_scaffolds.min1000.fasta
```

```{bash, eval = FALSE}
sed -i 's/>/>Brothers_LC_/g' S016_metaspades_scaffolds.min1000.fasta
```
### Brothers NWCA
```{bash, eval = FALSE}
sed -i 's/>/>Brothers_NWCA_/g' S013_metaspades_scaffolds.min1000.fasta
```

```{bash, eval = FALSE}
sed -i 's/>/>Brothers_NWCA_/g' S017_metaspades_scaffolds.min1000.fasta
```

```{bash, eval = FALSE}
sed -i 's/>/>Brothers_NWCA_/g' S142_metaspades_scaffolds.min1000.fasta
```

```{bash, eval = FALSE}
sed -i 's/>/>Brothers_NWCA_/g' S143_metaspades_scaffolds.min1000.fasta
```

```{bash, eval = FALSE}
sed -i 's/>/>Brothers_NWCA_/g' S144_metaspades_scaffolds.min1000.fasta
```

```{bash, eval = FALSE}
sed -i 's/>/>Brothers_NWCA_/g' S145_metaspades_scaffolds.min1000.fasta
```
### Brothers NWCB
```{bash, eval = FALSE}
sed -i 's/>/>Brothers_NWCB_/g' S012_metaspades_scaffolds.min1000.fasta
```

```{bash, eval = FALSE}
sed -i 's/>/>Brothers_NWCB_/g' S139_metaspades_scaffolds.min1000.fasta
```

```{bash, eval = FALSE}
sed -i 's/>/>Brothers_NWCB_/g' S140_metaspades_scaffolds.min1000.fasta
```

```{bash, eval = FALSE}
sed -i 's/>/>Brothers_NWCB_/g' S141_metaspades_scaffolds.min1000.fasta
```

```{bash, eval = FALSE}
sed -i 's/>/>Brothers_NWCB_/g' S146_metaspades_scaffolds.min1000.fasta
```
### Brothers Diffuse
```{bash, eval = FALSE}
sed -i 's/>/>Brothers_Diffuse_/g' S009_metaspades_scaffolds.min1000.fasta

sed -i 's/>/>Brothers_Diffuse_/g' S015_metaspades_scaffolds.min1000.fasta
```
### ELSC Abe
```{bash, eval = FALSE}
sed -i 's/>/>ELSC_Abe_/g' A1_metaspades_scaffolds.min1000.fasta

sed -i 's/>/>ELSC_Abe_/g' A3_metaspades_scaffolds.min1000.fasta

sed -i 's/>/>ELSC_Abe_/g' 128-326.fasta
```
### ELSC Tui Malila
```{bash, eval = FALSE}
sed -i 's/>/>ELSC_Tui_Malila_/g' T10_metaspades_scaffolds.min1000.fasta

sed -i 's/>/>ELSC_Tui_Malila_/g' T11_metaspades_scaffolds.min1000.fasta

sed -i 's/>/>ELSC_Tui_Malila_/g' T2_metaspades_scaffolds.min1000.fasta

sed -i 's/>/>ELSC_Tui_Malila_/g' 132-544.fasta

sed -i 's/>/>ELSC_Tui_Malila_/g' 134-614.fasta
```
### ELSC Bowl
```{bash, eval = FALSE}
sed -i 's/>/>ELSC_Bowl_/g' M1_metaspades_scaffolds.min1000.fasta

sed -i 's/>/>ELSC_Bowl_/g' M2_metaspades_scaffolds.min1000.fasta
```
### ELSC Mariner
```{bash, eval = FALSE}
sed -i 's/>/>ELSC_Mariner_/g' 131-447.fasta

sed -i 's/>/>ELSC_Mariner_/g' M10_metaspades_scaffolds.min1000.fasta

sed -i 's/>/>ELSC_Mariner_/g' M17_metaspades_scaffolds.min1000.fasta
```
### ELSC Vai Lili
```{bash, eval = FALSE}
sed -i 's/>/>ELSC_Vai_Lili_/g' V2_metaspades_scaffolds.min1000.fasta
```
### Guaymas
```{bash, eval = FALSE}
sed -i 's/>/>Guaymas_/g' 4559-240.fasta

sed -i 's/>/>Guaymas_/g' 4561-380.fasta

sed -i 's/>/>Guaymas_/g' 4561-384.fasta

sed -i 's/>/>Guaymas_/g' 4571-419.fasta
```
### EPR
```{bash, eval = FALSE}
sed -i 's/>/>EPR_/g' 4281-140.fasta

sed -i 's/>/>EPR_/g' PIR-30.fasta
```
### MAR Rainbow
```{bash, eval = FALSE}
sed -i 's/>/>MAR_Rainbow_/g' 354-166.fasta

sed -i 's/>/>MAR_Rainbow_/g' 355-202.fasta
```
### MAR Lucky
```{bash, eval = FALSE}
sed -i 's/>/>MAR_Lucky_/g' 356-284.fasta

sed -i 's/>/>MAR_Lucky_/g' 356-308.fasta
```

After scaffold renaming, I used `mv` commands to rename files.

# Identifying viruses using VIBRANT
VIBRANT filters to remove scaffolds <1kb so not filtering assembly before. ID's dsDNA, ssDNA, and RNA viruses
```{bash, eval = FALSE}
for i in Assemblies_Renamed/*.fasta; do VIBRANT_run.py -i $i -t 15; done
```

## Count the number of viruses identified by VIBRANT
```{bash, eval = FALSE}
grep ">" -c VIBRANT_*/VIBRANT_phages*/*combined.fna | cut -f2 -d : | paste -sd+ | bc
```
Total of 53,135 viruses identified

# Run CheckV
CheckV provides quality estimates of viral genomes. I'm running this first to determine how many medium and high confidence viruses we have. If it's a lot I'll only focus on these.
```{bash, eval = FALSE}
find -type f -name "*combined.fna" | xargs cp -t ../Virus_Genomes/fna/

nohup checkv end_to_end Virus_Genomes/fna/All_Viruses.fna CheckV_Output -t 20 &
```

Check how many medium and high quality viruses:
```{bash,  eval = FALSE}
grep -v 'Low-quality\|Not-determined' quality_summary.tsv | wc
```
Result: 1,165 medium and high quality viruses (exclude header)
146 complete viruses
1,031 proviruses, or integrated viruses

## Grab only the medium and high quality viruses
Grab only the medium and high quality viruses predicted by CheckV
```{bash,  eval = FALSE}
grep -v 'Low-quality\|Not-determined' quality_summary.tsv > MedHighQual_Viruses.tsv

cut -f1 MedHighQual_Viruses.tsv > MedHighQual_Viruses_list.txt 

perl /storage1/data12/scripts/screen_list_new.pl CheckV_Output/MedHighQual_Viruses_list.txt Virus_Genomes/fna/All_Viruses.fna keep > MedHighQual_Viruses.fna
```

Cp the faa viruses into a new folder

```{bash,  eval = FALSE}
find -type f -name "*combined.faa" | xargs cp -t ../Virus_Genomes/faa
```

Get a list of the faa scaffold names for medium and high quality viruses:
```{bash,  eval = FALSE}
for i in `cat ../../CheckV_Output/MedHighQual_Viruses_list.txt`; do grep $i *.faa >> MedHighQual_faa_list.txt; done

sed 's/>/\t/g' MedHighQual_faa_list.txt | cut -f2 > MedHighQual_faa_list_final.txt
```

Extract the faa seqs by name:
```{bash,  eval = FALSE}
perl /storage1/data12/scripts/screen_list_new.pl MedHighQual_faa_list_final.txt All_Viruses.faa keep > MedHighQual_Viruses.faa
```

# Organizing the MAGs
I want to associate the site names and file name of each MAG to the scaffold header. I am not being as careful about renaming for the MAGs as I was for the assemblies, meaning I am just using general site name and not specific (e.g. Brothers vs Brothers_UC).

## Brothers Volcano
```{bash,  eval = FALSE}
pwd

/storage1/data12/Projects/Vent_Viruses/MAGs_Renamed/Brothers_Volcano

rename 's/scaf2bin.//g' *.fasta

rename 's/^/Brothers_/g' *.fasta

for i in *.fasta; do awk '/>/{sub(">","&"FILENAME"_");sub(/\.fasta/,x)}1' $i >> $i.renamed; done
```

## ELSC
```{bash,  eval = FALSE}
rename 's/scaf2bin.//g' *.fasta

rename 's/^/ELSC_/g' *.fasta

for i in *.fasta; do awk '/>/{sub(">","&"FILENAME"_");sub(/\.fasta/,x)}1' $i >> $i.renamed; done
```

## EPR
```{bash,  eval = FALSE}
rename 's/scaf2bin.//g' *.fasta

rename 's/^/EPR_/g' *.fasta

for i in *.fasta; do awk '/>/{sub(">","&"FILENAME"_");sub(/\.fasta/,x)}1' $i >> $i.renamed; done
```

## Guaymas
```{bash,  eval = FALSE}
rename 's/scaf2bin.//g' *.fasta

rename 's/^/Guaymas_/g' *.fasta

for i in *.fasta; do awk '/>/{sub(">","&"FILENAME"_");sub(/\.fasta/,x)}1' $i >> $i.renamed; done
```

## MAR
```{bash,  eval = FALSE}
rename 's/scaf2bin.//g' *.fasta

rename 's/^/MAR_/g' *.fasta

for i in *.fasta; do awk '/>/{sub(">","&"FILENAME"_");sub(/\.fasta/,x)}1' $i >> $i.renamed; done
```

# Read Mapping

## EPR
```{bash,  eval = FALSE}
conda activate

nohup python3 /storage1/data14/software/mapping/mapper.py -i Assemblies_Renamed/EPR/EPR_4281-140.fasta -r /storage1/Reads/HydroVents_ZhouReysenbach/EPR/*.fastq.gz -o Read_Mapping/EPR -t 20 &

nohup python3 /storage1/data14/software/mapping/mapper.py -i Assemblies_Renamed/EPR/EPR_PIR-30.fasta -r /storage1/Reads/HydroVents_ZhouReysenbach/EPR/*.fastq.gz -o Read_Mapping/EPR -t 20 &
```

## Guaymas
```{bash,  eval = FALSE}
nohup python3 /storage1/data14/software/mapping/mapper.py -i Assemblies_Renamed/Guaymas/Guaymas_4559-240.fasta -r /storage1/Reads/HydroVents_ZhouReysenbach/Guaymas/*.fastq.gz -o Read_Mapping/Guaymas -t 20 &

nohup python3 /storage1/data14/software/mapping/mapper.py -i Assemblies_Renamed/Guaymas/Guaymas_4561-380.fasta -r /storage1/Reads/HydroVents_ZhouReysenbach/Guaymas/*.fastq.gz -o Read_Mapping/Guaymas -t 20 &

nohup python3 /storage1/data14/software/mapping/mapper.py -i Assemblies_Renamed/Guaymas/Guaymas_4561-384.fasta -r /storage1/Reads/HydroVents_ZhouReysenbach/Guaymas/*.fastq.gz -o Read_Mapping/Guaymas -t 20 &

nohup python3 /storage1/data14/software/mapping/mapper.py -i Assemblies_Renamed/Guaymas/Guaymas_4571-419.fasta -r /storage1/Reads/HydroVents_ZhouReysenbach/Guaymas/*.fastq.gz -o Read_Mapping/Guaymas -t 20 &
```

## MAR

```{bash,  eval = FALSE}
nohup python3 /storage1/data14/software/mapping/mapper.py -i Assemblies_Renamed/MAR_Lucky_356-284.fasta -r /storage1/Reads/HydroVents_ZhouReysenbach/MAR/*.fastq.gz -o Read_Mapping/MAR -t 20 &

nohup python3 /storage1/data14/software/mapping/mapper.py -i Assemblies_Renamed/MAR_Lucky_356-308.fasta -r /storage1/Reads/HydroVents_ZhouReysenbach/MAR/*.fastq.gz -o Read_Mapping/MAR -t 15 &

nohup python3 /storage1/data14/software/mapping/mapper.py -i Assemblies_Renamed/MAR_Rainbow_354-166.fasta -r /storage1/Reads/HydroVents_ZhouReysenbach/MAR/*.fastq.gz -o Read_Mapping/MAR -t 20 &

nohup python3 /storage1/data14/software/mapping/mapper.py -i Assemblies_Renamed/MAR_Rainbow_355-202.fasta -r /storage1/Reads/HydroVents_ZhouReysenbach/MAR/*.fastq.gz -o Read_Mapping/MAR -t 20 &
```

## Brothers and ELSC
Now trying with Cody's mapping wrapper
```{bash,  eval = FALSE}
conda activate

nohup python3 /storage1/data14/software/mapping/mapper.py -i AssembliesToReadsFinal.txt --batch -o Read_Mapping/BrothersELSC -rd /storage1/Reads/HydroVents_ZhouReysenbach -fd /storage1/data12/Projects/Vent_Viruses/Assemblies_Renamed -t 20 &
```

# Remove viral contamination from MAGs

BLAST all viruses against all the MAGs. You are looking for 100% matches and 100% coverage. Unless it is a lysogenic virus, it should be contamination. Just because a virus is binned with a MAG does not mean it belongs to that virus.

1. Run BLAST search
```{bash,  eval = FALSE}
cat *.fasta > All_VentMAGs.fna

makeblastdb -in All_VentMAGs.fna -title "Vent_MAGs" -dbtype nucl

nohup blastn -query  /storage1/data12/Projects/Vent_Viruses/Virus_Genomes/fna/All_Viruses.fna -db /storage1/data12/Projects/Vent_Viruses/MAGs_Renamed/Remove_ViralContam/All_VentMAGs.fna -out MAGs_Renamed/Remove_ViralContam/VirusHits_toMAGs.txt -num_threads 20 -outfmt "6 qseqid length qlen slen pident bitscore stitle" &
```

2. Filter blast hits and subset from MAGs
```{bash,  eval = FALSE}
python3 /storage1/data12/scripts/filter_blast_100c_pi.py VirusHits_toMAGs.txt VirusHits_toMAGs_filtered.txt

cut -f7 VirusHits_toMAGs_filtered.txt > Viral_Contam.txt

for i in *.fasta; do perl /storage1/data12/scripts/screen_list_new.pl Remove_ViralContam/Viral_Contam.txt $i >> Remove_ViralContam/$i.noVirusContam.fna; done

cat *noVirusContam* > All_VentMAGs_noViralContam.fna
```

# ViWrap
**Unfortunately this ended up not being an option because it would take a month to run through all my samples. One smaller scale sample took 10 hours, times 42 samples = inefficient.**

Testing ViWrap - it would be nice if I could use it to get all the output I need :)

## Brothers Diffuse
```{bash,  eval = FALSE}
conda activate /storage1/data22/ViWrap_conda_environments/ViWrap

ViWrap run --input_metagenome /storage1/data12/Projects/Vent_Viruses/Assemblies_Renamed/Brothers_Volcano/Brothers_Diffuse_S009_metaspades_scaffolds.min1000.fasta \
               --input_reads /storage1/Reads/HydroVents_ZhouReysenbach/Brothers_Volcano/Diffuse_Flow/lane2-s009-indexN705-S505-GGACTCCT-CTCCTTAC-279_debarcode_1.fastq.gz,/storage1/Reads/HydroVents_ZhouReysenbach/Brothers_Volcano/Diffuse_Flow/lane2-s009-indexN705-S505-GGACTCCT-CTCCTTAC-279_debarcode_2.fastq.gz,/storage1/Reads/HydroVents_ZhouReysenbach/Brothers_Volcano/Diffuse_Flow/lane4-s015-indexN703-S517-AGGCAGAA-TCTTACGC-Green_debarcode_1.fastq.gz,/storage1/Reads/HydroVents_ZhouReysenbach/Brothers_Volcano/Diffuse_Flow/lane4-s015-indexN703-S517-AGGCAGAA-TCTTACGC-Green_debarcode_2.fastq.gz \
               --input_reads_type illumina \
               --reads_mapping_identity_cutoff 0.97 \
               --out_dir /storage1/data12/Projects/Vent_Viruses/ViWrap/Brothers_Diffuse \
               --db_dir /storage1/data22/ViWrap/ViWrap_db \
               --identify_method  vb \
               --conda_env_dir /storage1/data22/ViWrap_conda_environments \
               --threads 20 \
               --input_length_limit 2000 \
               --custom_MAGs_dir /storage1/data12/Projects/Vent_Viruses/MAGs_Renamed/Remove_ViralContam/Brothers/Diffuse
               

nohup ./bash_BD.sh &
```

I decided to leave off the custom MAG option for iphop since that takes a lot of time. I can take all the viruses and all the MAGs and run that on my own after I have all the vMAGs and unbinned viruses.

```{bash,  eval = FALSE}
conda activate /storage1/data22/ViWrap_conda_environments/ViWrap

ViWrap run --input_metagenome /storage1/data12/Projects/Vent_Viruses/Assemblies_Renamed/Brothers_Volcano/Brothers_Diffuse_S015_metaspades_scaffolds.min1000.fasta \
               --input_reads /storage1/Reads/HydroVents_ZhouReysenbach/Brothers_Volcano/Diffuse_Flow/lane2-s009-indexN705-S505-GGACTCCT-CTCCTTAC-279_debarcode_1.fastq.gz,/storage1/Reads/HydroVents_ZhouReysenbach/Brothers_Volcano/Diffuse_Flow/lane2-s009-indexN705-S505-GGACTCCT-CTCCTTAC-279_debarcode_2.fastq.gz,/storage1/Reads/HydroVents_ZhouReysenbach/Brothers_Volcano/Diffuse_Flow/lane4-s015-indexN703-S517-AGGCAGAA-TCTTACGC-Green_debarcode_1.fastq.gz,/storage1/Reads/HydroVents_ZhouReysenbach/Brothers_Volcano/Diffuse_Flow/lane4-s015-indexN703-S517-AGGCAGAA-TCTTACGC-Green_debarcode_2.fastq.gz \
               --input_reads_type illumina \
               --reads_mapping_identity_cutoff 0.97 \
               --out_dir /storage1/data12/Projects/Vent_Viruses/ViWrap/Brothers_Diffuse_S015 \
               --db_dir /storage1/data22/ViWrap/ViWrap_db \
               --identify_method  vb \
               --conda_env_dir /storage1/data22/ViWrap_conda_environments \
               --threads 20 \
               --input_length_limit 2000
               

nohup ./bash_BD_S015.sh &
```

## Running ViWrap using Cody's wrapper

```{bash,  eval = FALSE}
conda activate

nohup python3 /storage1/data14/for_maggie/ViWrap_loop.py -s AssembliesToReads_ViWrap.txt -a /storage1/data12/Projects/Vent_Viruses/Assemblies_Renamed/ -r /storage1/Reads/HydroVents_ZhouReysenbach/ &
```





# vRhyme Binning

Don't judge me for running this one by one...I was doing it as the bam files were coming out from the read mapping because I'm impatient :)

## EPR
```{bash,  eval = FALSE}
conda activate vRhyme

vRhyme -i VIBRANT_output/VIBRANT_EPR_4281-140/VIBRANT_phages_EPR_4281-140/EPR_4281-140.phages_combined.fna -b Read_Mapping/EPR/EPR_4281-140/*.bam -t 5

vRhyme -i VIBRANT_output/VIBRANT_EPR_PIR-30/VIBRANT_phages_EPR_PIR-30/EPR_PIR-30.phages_combined.fna -b Read_Mapping/EPR/EPR_PIR-30/*.bam -t 5
```

## Guaymas
```{bash,  eval = FALSE}
conda activate vRhyme

vRhyme -i VIBRANT_output/VIBRANT_Guaymas_4559-240/VIBRANT_phages_Guaymas_4559-240/Guaymas_4559-240.phages_combined.fna   -b Read_Mapping/Guaymas/Guaymas_4559-240/*.bam -t 5

vRhyme -i VIBRANT_output/VIBRANT_Guaymas_4561-380/VIBRANT_phages_Guaymas_4561-380/Guaymas_4561-380.phages_combined.fna  -b Read_Mapping/Guaymas/Guaymas_4561-380/*.bam -t 5

vRhyme -i VIBRANT_output/VIBRANT_Guaymas_4561-384/VIBRANT_phages_Guaymas_4561-384/Guaymas_4561-384.phages_combined.fna -b Read_Mapping/Guaymas/Guaymas_4561-384/*.bam -t 5

vRhyme -i VIBRANT_output/VIBRANT_Guaymas_4571-419/VIBRANT_phages_Guaymas_4571-419/Guaymas_4571-419.phages_combined.fna  -b Read_Mapping/Guaymas/Guaymas_4571-419/*.bam -t 5
```

## MAR
```{bash,  eval = FALSE}
conda activate vRhyme

vRhyme -i VIBRANT_output/VIBRANT_MAR_Lucky_356-284/VIBRANT_phages_MAR_Lucky_356-284/MAR_Lucky_356-284.phages_combined.fna  -b Read_Mapping/MAR/MAR_Lucky_356-284/*.bam -t 5

vRhyme -i VIBRANT_output/VIBRANT_MAR_Lucky_356-308/VIBRANT_phages_MAR_Lucky_356-308/MAR_Lucky_356-308.phages_combined.fna -b Read_Mapping/MAR/MAR_Lucky_356-308/*.bam -t 5

vRhyme -i VIBRANT_output/VIBRANT_MAR_Rainbow_354-166/VIBRANT_phages_MAR_Rainbow_354-166/MAR_Rainbow_354-166.phages_combined.fna -b Read_Mapping/MAR/MAR_Rainbow_354-166/*.bam -t 5

vRhyme -i VIBRANT_output/VIBRANT_MAR_Rainbow_355-202/VIBRANT_phages_MAR_Rainbow_355-202/MAR_Rainbow_355-202.phages_combined.fna -b Read_Mapping/MAR/MAR_Rainbow_355-202/*.bam -t 5
```



## Brothers
### Diffuse
```{bash,  eval = FALSE}
conda activate vRhyme

vRhyme -i VIBRANT_output/VIBRANT_Brothers_Diffuse_S009_metaspades_scaffolds.min1000/VIBRANT_phages_Brothers_Diffuse_S009_metaspades_scaffolds.min1000/Brothers_Diffuse_S009_metaspades_scaffolds.min1000.phages_combined.fna -b Read_Mapping/BrothersELSC/Brothers_Diffuse_S009* -t 5

vRhyme -i VIBRANT_output/VIBRANT_Brothers_Diffuse_S015_metaspades_scaffolds.min1000/VIBRANT_phages_Brothers_Diffuse_S015_metaspades_scaffolds.min1000/Brothers_Diffuse_S015_metaspades_scaffolds.min1000.phages_combined.fna -b Read_Mapping/BrothersELSC/Brothers_Diffuse_S015/*bam -t 5
```

### LC
```{bash,  eval = FALSE}
vRhyme -i VIBRANT_output/VIBRANT_Brothers_LC_S014_metaspades_scaffolds.min1000/VIBRANT_phages_Brothers_LC_S014_metaspades_scaffolds.min1000/Brothers_LC_S014_metaspades_scaffolds.min1000.phages_combined.fna -b Read_Mapping/BrothersELSC/Brothers_LC_S014/*bam -t 5

vRhyme -i VIBRANT_output/VIBRANT_Brothers_LC_S016_metaspades_scaffolds.min1000/VIBRANT_phages_Brothers_LC_S016_metaspades_scaffolds.min1000/Brothers_LC_S016_metaspades_scaffolds.min1000.phages_combined.fna -b Read_Mapping/BrothersELSC/Brothers_LC_S016/*bam -t 5
```

### NWCA
```{bash,  eval = FALSE}
vRhyme -i VIBRANT_output/VIBRANT_Brothers_NWCA_S013_metaspades_scaffolds.min1000/VIBRANT_phages_Brothers_NWCA_S013_metaspades_scaffolds.min1000/Brothers_NWCA_S013_metaspades_scaffolds.min1000.phages_combined.fna -b Read_Mapping/BrothersELSC/Brothers_NWCA_S013/*bam -t 5

vRhyme -i VIBRANT_output/VIBRANT_Brothers_NWCA_S017_metaspades_scaffolds.min1000/VIBRANT_phages_Brothers_NWCA_S017_metaspades_scaffolds.min1000/Brothers_NWCA_S017_metaspades_scaffolds.min1000.phages_combined.fna -b Read_Mapping/BrothersELSC/Brothers_NWCA_S017/*bam -t 5

vRhyme -i VIBRANT_output/VIBRANT_Brothers_NWCA_S142_metaspades_scaffolds.min1000/VIBRANT_phages_Brothers_NWCA_S142_metaspades_scaffolds.min1000/Brothers_NWCA_S142_metaspades_scaffolds.min1000.phages_combined.fna -b Read_Mapping/BrothersELSC/Brothers_NWCA_S142/*bam -t 5

vRhyme -i VIBRANT_output/VIBRANT_Brothers_NWCA_S143_metaspades_scaffolds.min1000/VIBRANT_phages_Brothers_NWCA_S143_metaspades_scaffolds.min1000/Brothers_NWCA_S143_metaspades_scaffolds.min1000.phages_combined.fna -b Read_Mapping/BrothersELSC/Brothers_NWCA_S143/*bam -t 5

vRhyme -i VIBRANT_output/VIBRANT_Brothers_NWCA_S144_metaspades_scaffolds.min1000/VIBRANT_phages_Brothers_NWCA_S144_metaspades_scaffolds.min1000/Brothers_NWCA_S144_metaspades_scaffolds.min1000.phages_combined.fna -b Read_Mapping/BrothersELSC/Brothers_NWCA_S144/*bam -t 5

vRhyme -i VIBRANT_output/VIBRANT_Brothers_NWCA_S145_metaspades_scaffolds.min1000/VIBRANT_phages_Brothers_NWCA_S145_metaspades_scaffolds.min1000/Brothers_NWCA_S145_metaspades_scaffolds.min1000.phages_combined.fna -b Read_Mapping/BrothersELSC/Brothers_NWCA_S145/*bam -t 5
```


### NWCB
```{bash,  eval = FALSE}
vRhyme -i VIBRANT_output/VIBRANT_Brothers_NWCB_S012_metaspades_scaffolds.min1000/VIBRANT_phages_Brothers_NWCB_S012_metaspades_scaffolds.min1000/Brothers_NWCB_S012_metaspades_scaffolds.min1000.phages_combined.fna -b Read_Mapping/BrothersELSC/Brothers_NWCB_S012/*bam -t 5

vRhyme -i VIBRANT_output/VIBRANT_Brothers_NWCB_S139_metaspades_scaffolds.min1000/VIBRANT_phages_Brothers_NWCB_S139_metaspades_scaffolds.min1000/Brothers_NWCB_S139_metaspades_scaffolds.min1000.phages_combined.fna -b Read_Mapping/BrothersELSC/Brothers_NWCB_S139/*bam -t 5

vRhyme -i VIBRANT_output/VIBRANT_Brothers_NWCB_S140_metaspades_scaffolds.min1000/VIBRANT_phages_Brothers_NWCB_S140_metaspades_scaffolds.min1000/Brothers_NWCB_S140_metaspades_scaffolds.min1000.phages_combined.fna -b Read_Mapping/BrothersELSC/Brothers_NWCB_S140/*bam -t 5

vRhyme -i VIBRANT_output/VIBRANT_Brothers_NWCB_S141_metaspades_scaffolds.min1000/VIBRANT_phages_Brothers_NWCB_S141_metaspades_scaffolds.min1000/Brothers_NWCB_S141_metaspades_scaffolds.min1000.phages_combined.fna -b Read_Mapping/BrothersELSC/Brothers_NWCB_S141/*bam -t 5

vRhyme -i VIBRANT_output/VIBRANT_Brothers_NWCB_S146_metaspades_scaffolds.min1000/VIBRANT_phages_Brothers_NWCB_S146_metaspades_scaffolds.min1000/Brothers_NWCB_S146_metaspades_scaffolds.min1000.phages_combined.fna -b Read_Mapping/BrothersELSC/Brothers_NWCB_S146/*bam -t 5
```

### UC
```{bash,  eval = FALSE}
vRhyme -i VIBRANT_output/VIBRANT_Brothers_UC_S010_metaspades_scaffolds.min1000/VIBRANT_phages_Brothers_UC_S010_metaspades_scaffolds.min1000/Brothers_UC_S010_metaspades_scaffolds.min1000.phages_combined.fna -b Read_Mapping/BrothersELSC/Brothers_UC_S010/*bam -t 5

vRhyme -i VIBRANT_output/VIBRANT_Brothers_UC_S011_metaspades_scaffolds.min1000/VIBRANT_phages_Brothers_UC_S011_metaspades_scaffolds.min1000/Brothers_UC_S011_metaspades_scaffolds.min1000.phages_combined.fna -b Read_Mapping/BrothersELSC/Brothers_UC_S011/*bam -t 5

vRhyme -i VIBRANT_output/VIBRANT_Brothers_UC_S147_metaspades_scaffolds.min1000/VIBRANT_phages_Brothers_UC_S147_metaspades_scaffolds.min1000/Brothers_UC_S147_metaspades_scaffolds.min1000.phages_combined.fna -b Read_Mapping/BrothersELSC/Brothers_UC_S147/*bam -t 5
```

### ELSC Abe
```{bash,  eval = FALSE}
vRhyme -i VIBRANT_output/VIBRANT_ELSC_Abe_128-326/VIBRANT_phages_ELSC_Abe_128-326/ELSC_Abe_128-326.phages_combined.fna -b Read_Mapping/BrothersELSC/ELSC_Abe_128-326/*bam -t 5

vRhyme -i VIBRANT_output/VIBRANT_ELSC_Abe_A1_metaspades_scaffolds.min1000/VIBRANT_phages_ELSC_Abe_A1_metaspades_scaffolds.min1000/ELSC_Abe_A1_metaspades_scaffolds.min1000.phages_combined.fna -b Read_Mapping/BrothersELSC/ELSC_Abe_A1/*bam -t 5

vRhyme -i VIBRANT_output/VIBRANT_ELSC_Abe_A3_metaspades_scaffolds.min1000/VIBRANT_phages_ELSC_Abe_A3_metaspades_scaffolds.min1000/ELSC_Abe_A3_metaspades_scaffolds.min1000.phages_combined.fna -b Read_Mapping/BrothersELSC/ELSC_Abe_A3/*bam -t 5
```
### ELSC Mariner
```{bash,  eval = FALSE}
vRhyme -i VIBRANT_output/VIBRANT_ELSC_Bowl_M1_metaspades_scaffolds.min1000/VIBRANT_phages_ELSC_Bowl_M1_metaspades_scaffolds.min1000/ELSC_Bowl_M1_metaspades_scaffolds.min1000.phages_combined.fna -b Read_Mapping/BrothersELSC/ELSC_Bowl_M1/*bam -t 5

vRhyme -i VIBRANT_output/VIBRANT_ELSC_Bowl_M2_metaspades_scaffolds.min1000/VIBRANT_phages_ELSC_Bowl_M2_metaspades_scaffolds.min1000/ELSC_Bowl_M2_metaspades_scaffolds.min1000.phages_combined.fna -b Read_Mapping/BrothersELSC/ELSC_Bowl_M2/*bam -t 5

vRhyme -i VIBRANT_output/VIBRANT_ELSC_Mariner_131-447/VIBRANT_phages_ELSC_Mariner_131-447/ELSC_Mariner_131-447.phages_combined.fna -b Read_Mapping/BrothersELSC/ELSC_Mariner_131-447/*bam -t 5

vRhyme -i VIBRANT_output/VIBRANT_ELSC_Mariner_M10_metaspades_scaffolds.min1000/VIBRANT_phages_ELSC_Mariner_M10_metaspades_scaffolds.min1000/ELSC_Mariner_M10_metaspades_scaffolds.min1000.phages_combined.fna -b Read_Mapping/BrothersELSC/ELSC_Mariner_M10/*bam -t 5

vRhyme -i VIBRANT_output/VIBRANT_ELSC_Mariner_M17_metaspades_scaffolds.min1000/VIBRANT_phages_ELSC_Mariner_M17_metaspades_scaffolds.min1000/ELSC_Mariner_M17_metaspades_scaffolds.min1000.phages_combined.fna -b Read_Mapping/BrothersELSC/ELSC_Mariner_M17/*bam -t 5
```

```{bash,  eval = FALSE}
vRhyme -i VIBRANT_output/VIBRANT_ELSC_Abe_128-326/VIBRANT_phages_ELSC_Abe_128-326/ELSC_Abe_128-326.phages_combined.fna -b Read_Mapping/BrothersELSC/ELSC_Abe_128-326/*bam -t 5

vRhyme -i VIBRANT_output/VIBRANT_ELSC_Abe_A1_metaspades_scaffolds.min1000/VIBRANT_phages_ELSC_Abe_A1_metaspades_scaffolds.min1000/ELSC_Abe_A1_metaspades_scaffolds.min1000.phages_combined.fna -b Read_Mapping/BrothersELSC/ELSC_Abe_A1/*bam -t 5

vRhyme -i VIBRANT_output/VIBRANT_ELSC_Abe_A3_metaspades_scaffolds.min1000/VIBRANT_phages_ELSC_Abe_A3_metaspades_scaffolds.min1000/ELSC_Abe_A3_metaspades_scaffolds.min1000.phages_combined.fna -b Read_Mapping/BrothersELSC/ELSC_Abe_A3/*bam -t 5
```


### ELSC Tui Malila
```{bash,  eval = FALSE}
vRhyme -i VIBRANT_output/VIBRANT_ELSC_Tui_Malila_132-544/VIBRANT_phages_ELSC_Tui_Malila_132-544/ELSC_Tui_Malila_132-544.phages_combined.fna -b Read_Mapping/BrothersELSC/ELSC_Tui_Malila_132-544/*bam -t 5

vRhyme -i VIBRANT_output/VIBRANT_ELSC_Tui_Malila_134-614/VIBRANT_phages_ELSC_Tui_Malila_134-614/ELSC_Tui_Malila_134-614.phages_combined.fna -b Read_Mapping/BrothersELSC/ELSC_Tui_Malila_134-614/*bam -t 5

vRhyme -i VIBRANT_output/VIBRANT_ELSC_Tui_Malila_T10_metaspades_scaffolds.min1000/VIBRANT_phages_ELSC_Tui_Malila_T10_metaspades_scaffolds.min1000/ELSC_Tui_Malila_T10_metaspades_scaffolds.min1000.phages_combined.fna -b Read_Mapping/BrothersELSC/ELSC_Tui_Malila_T10/*bam -t 5

vRhyme -i VIBRANT_output/VIBRANT_ELSC_Tui_Malila_T11_metaspades_scaffolds.min1000/VIBRANT_phages_ELSC_Tui_Malila_T11_metaspades_scaffolds.min1000/ELSC_Tui_Malila_T11_metaspades_scaffolds.min1000.phages_combined.fna -b Read_Mapping/BrothersELSC/ELSC_Tui_Malila_T11/*bam -t 5

vRhyme -i VIBRANT_output/VIBRANT_ELSC_Tui_Malila_T2_metaspades_scaffolds.min1000/VIBRANT_phages_ELSC_Tui_Malila_T2_metaspades_scaffolds.min1000/ELSC_Tui_Malila_T2_metaspades_scaffolds.min1000.phages_combined.fna -b Read_Mapping/BrothersELSC/ELSC_Tui_Malila_T2/*bam -t 5
```

### Vai Lili
```{bash,  eval = FALSE}
vRhyme -i VIBRANT_output/VIBRANT_ELSC_Vai_Lili_V2_metaspades_scaffolds.min1000/VIBRANT_phages_ELSC_Vai_Lili_V2_metaspades_scaffolds.min1000/ELSC_Vai_Lili_V2_metaspades_scaffolds.min1000.phages_combined.fna -b Read_Mapping/BrothersELSC/ELSC_Vai_Lili_V2/*bam -t 5
```


# Renaming vMAGs
```{bash,  eval = FALSE}
mkdir originals_vMAGs
```

1. Run bash script with the following contents to find vMAGs, rename them, cp them 1 dir up:
```{bash,  eval = FALSE}
./rename_vMAGs.sh

#!/bin/bash

find . -maxdepth 3 -type f -name "*fasta" -exec sh -c '
  for img; do
    parentdir=${img%/*/*}      # leave the parent dir (remove the last `/` and filename)
    dirname=${parentdir##*/} # leave the parent directory name (remove all parent paths `*/`)
    cp -i "$img" "$parentdir/${dirname}_${img##*/}"
  done
' sh {} +
```

2. Find renamed vMAGs, put them in 1 folder, clean up the names
```{bash,  eval = FALSE}
find -maxdepth 2 -name "*.fasta" | xargs -i mv {} fna/

rename 's/vRhyme_results_//' *.fasta
rename 's/.phages_combined//' *.fasta
rename 's/_metaspades_scaffolds.min1000//' *.fasta
```

# Use vRhyme to get the N-linked scaffolds of vMAGs
You'll need this to run CheckV, iphop, and other programs on the vMAGs

```{bash,  eval = FALSE}
conda activate vRhyme

link_bin_sequences.py -i fna/ -o fna_Nlinked_scaffolds
```

# Get a file of unbinned viruses for the same 42 samples

1. Concatenate the viruses into 1 file
```{bash,  eval = FALSE}
cat *fasta > ../all_vMAGs.fna
```

2. Make a mapping file to use in the future for mapping vMAG file name: scaffold names
```{bash,  eval = FALSE}
grep ">" *.fasta | sed 's/:>/\t/g' | sed 's/.fasta/\t/g' | cut -f1,3 | sed 's/__/\t/g' | cut -f1,3 > ../vMAG_scaffold_mapping.txt

cut -f2 vMAG_scaffold_mapping.txt > binned_viral_scaffolds.txt
```

3. Subset the file of all viruses for unbinned viruses using the txt file of binned viral scaffolds
```{bash,  eval = FALSE}
screen_list_new.pl ../../../vRhyme/binned_viral_scaffolds.txt All_Viruses.fna > unbinned_VentViruses.fna
```

# Run iphop on the vUnbinned and vMAGs
Run iphop on the 1500N-concatenated vMAGs and unbinned viruses so you can compare host predictions with the plume viruses.

**If you're doing this again, do step 2 first if you anticipate any illegal characters in your MAGs. Then proceed**

First, make a custom iphop database using the MAGs from the 42 vents
1. Run gtdbtk 1.5.0 on the MAGs
```{bash,  eval = FALSE}
conda activate gtdbtk-1.5.0

nohup gtdbtk de_novo_wf --genome_dir MAGs_Renamed/No_ViralContam/ --bacteria --outgroup_taxon p__Bdellovibrionota --out_dir Vent_MAGs_gtdbtk_denovo_wf/ --cpus 20 --force --extension fna &

nohup gtdbtk de_novo_wf --genome_dir MAGs_Renamed/No_ViralContam/ --archaea --outgroup_taxon p__Altarchaeota --out_dir Vent_MAGs_gtdbtk_denovo_wf/ --cpus 20 --force --extension fna &
```

2. Create iphop database. I ran into the same problem that James did where iphop complained about illegal characters so I'm going to follow his steps to fix it. In the second command we are running iphop predict with the MAGs as the input viruses to get the "cleaned file" since my MAGs are giving me errors for illegal characters. Then use the split_MAGs.py script to split up the MAGs back into individual files.
```{bash,  eval = FALSE}
conda activate iphop

nohup iphop predict --fa_file MAGs_Renamed/All_VentMAGs_noViralContam.fna --out_dir clean_iphop_MAGs --db_dir /storage1/databases/iPHoP/Sept_2021_pub/ --num_threads 20 &

python3 split_MAGs.py
```

3. Rerun GTDBtk stuff with new names/cleaned MAGs. Remember the outgroups should be something you do not have in your dataset. So remember to check the GTDBtk taxonomy of your MAGs, then choose an outgroup that is distant.
```{bash,  eval = FALSE}
conda activate gtdbtk-1.5.0

nohup gtdbtk de_novo_wf --genome_dir clean_iphop_MAGs/clean_fnas/ --bacteria --outgroup_taxon p__Bdellovibrionota --out_dir Vent_MAGs_gtdbtk_denovo_wf_clean/ --cpus 20 &

nohup gtdbtk de_novo_wf --genome_dir clean_iphop_MAGs/clean_fnas/ --archaea --outgroup_taxon p__Huberarchaeota --out_dir Vent_MAGs_gtdbtk_denovo_wf_clean/ --cpus 20 &
```

4. Add MAGs finally to iphop db
```{bash,  eval = FALSE}
nohup iphop add_to_db --fna_dir clean_iphop_MAGs/clean_fnas/ --gtdb_dir Vent_MAGs_gtdbtk_denovo_wf_clean/ --out_dir iPHoP_Sept_2021_w_VentMAGs --db_dir /storage1/databases/iPHoP/Sept_2021_pub/ &
```

5. Run iphop using your custom database
```{bash,  eval = FALSE}
nohup iphop predict --fa_file Virus_Genomes/fna/vUnbinned_vMAG_1500Ns_VentViruses.fna --db_dir iPHoP_Sept_2021_w_VentMAGs/ --out_dir iphop/iphop_vUnbinned_vMAG_1500Ns_MAGdb -t 20 --no_qc &
```

# Collect the VIBRANT output into single files
```{bash,  eval = FALSE}
find VIBRANT* -name "*AMG_individuals*" | xargs -i cp {} AMG_VIBRANT_output/AMG_individuals/

find VIBRANT* -name "*AMG_counts*" | xargs -i cp {} AMG_VIBRANT_output/AMG_counts/

find VIBRANT* -name "*annotations*" | xargs -i cp {} AMG_VIBRANT_output/AMG_counts/

find VIBRANT* -name "*genome_quality*" | xargs -i cp {} Quality_VIBRANT_output/
```

# Run CheckV on the vMAGs

```{bash, eval = FALSE}
nohup checkv end_to_end Virus_Genomes/fna/all_vMAGs_1500Ns.fna CheckV_vMAGs -t 20 &
```

## Make a file of unbinned CheckV results

```{bash, eval = FALSE}
screen_list_new.pl ../vRhyme/binned_viral_scaffolds.txt quality_summary.tsv > quality_summary_vUnbinned_Vents.tsv
```
I ended up doing this quickly with vlookup in excel because this was not behaving as expected/not working

# Rename the faa vMAG files

1. Modify the bash script to find the faa vMAGs
```{bash, eval = FALSE}
./rename_vMAGs.sh

#!/bin/bash

find . -maxdepth 3 -type f -name "*[0-9].faa" -exec sh -c '
  for img; do
    parentdir=${img%/*/*}      # leave the parent dir (remove the last `/` and filename)
    dirname=${parentdir##*/} # leave the parent directory name (remove all parent paths `*/`)
    cp -i "$img" "$parentdir/${dirname}_${img##*/}"
  done
' sh {} +
```

2. Mv them to 1 dir
```{bash, eval = FALSE}
find -maxdepth 2 -name "*[0-9].faa" | xargs -i mv {} /storage1/data12/Projects/Vent_Viruses/vRhyme/faa/
```

3. Clean up the names
```{bash, eval = FALSE}
rename 's/vRhyme_results_//' *.faa
rename 's/.phages_combined//' *.faa
rename 's/_metaspades_scaffolds.min1000//' *.faa
```

# Clustering
## Using skani to compare ANI between viruses

This utilizes a skani-vMAG.py script written by Cody.
```{bash, eval = FALSE}
conda install -c bioconda skani

nohup /storage1/data14/software/skani-vMAG/python/skani-vMAG.py -c ../Virus_Genomes/fna/unbinned_VentViruses.fna -d ../vRhyme/fna -x .fasta --outdir . &
```

## Cluster the skani results using mcl

1. Preprocess the file using polars. This utilizes preprocess.py written by Cody.
```{bash, eval = FALSE}
conda create --name mcl
conda install -c "bioconda/label/cf201901" mcl
conda install -c conda-forge polars

/storage1/data14/software/skani-vMAG/python/preprocess.py -i skani_output_vUnbinned_vMAGs_Vents.tsv -o skani_PreprocessedOutput_vUnbinned_vMAGs_Vents.tsv
```

2. Pass the preprocessed file to mcxload
```{bash, eval = FALSE}
mcxload -abc skani/skani_PreprocessedOutput_vUnbinned_vMAGs_Vents.tsv -o mcl_clusters/vUnbinned_vMAGs_Vents.mci -write-tab mcl_clusters/vUnbinned_vMAGs_Vents.mcxload
```

3. Then to mcl
```{bash, eval = FALSE}
nohup mcl vUnbinned_vMAGs_Vents.mci -use-tab vUnbinned_vMAGs_Vents.mcxload -o out_vUnbinned_vMAGs_Vents.clusters &
```
That pipeline seems to work and I get 2,574 clusters. That seems reasonable for 31,136 viruses. The clusters look mixed by vent.

## ANI clustering with skani and mcl using both Vent and Plume data

1. Compare ANI using skani
```{bash, eval = FALSE}
conda activate

nohup /storage1/data14/software/skani-vMAG/python/skani-vMAG.py -c fna_vUnbinned/unbinned_PlumeVent_viruses.fna -d fna_vMAGs -x .fasta --outdir . -o skani_ANI_vUnbinned_vMAGs_PlumeVents.tsv &
```

2. Preprocess
```{bash, eval = FALSE}
/storage1/data14/software/skani-vMAG/python/preprocess.py -i ../../skani/PlumeVent/skani_ANI_vUnbinned_vMAGs_PlumeVents.tsv -o skani_ANI_vUnbinned_vMAGs_PlumeVents_processed.tsv
```

3. MCL
```{bash, eval = FALSE}
conda activate mcl

mcxload -abc skani_ANI_vUnbinned_vMAGs_PlumeVents_processed.tsv -o vUnbinned_vMAGs_PlumeVents.mci -write-tab vUnbinned_vMAGs_PlumeVents.mcxload

nohup mcl vUnbinned_vMAGs_PlumeVents.mci -use-tab vUnbinned_vMAGs_PlumeVents.mcxload -o vUnbinned_vMAGs_PlumeVents.clusters &
```
This produced 3,294 clusters from 38,014 viruses

4. Analyze cluster output
First convert file to comma separated for easier data wrangling.
```{bash, eval = FALSE}
sed 's/\t/,/g' vUnbinned_vMAGs_PlumeVents.clusters > vUnbinned_vMAGs_PlumeVents.clusters.csv
```

Grab only comma separated lines to see how many clusters excluding singletons.
```{bash, eval = FALSE}
grep "," vUnbinned_vMAGs_PlumeVents.clusters.csv > vUnbinned_vMAGs_PlumeVents.clusters_noSingletons.csv

cat vUnbinned_vMAGs_PlumeVents.clusters_noSingletons.csv | wc
```
**2,797 clusters with 2 or more viral genomes. 497 viral genomes are singletons**

### To reformat mcl cluster table so that vRhyme MAG names are in the right format

```{bash, eval = FALSE}
cut -f1,2 mcl_formatted_table_VentPlumeViruses.tsv | sort | sed 's/__/\t/g' | sed 's/vRhyme_/vRhyme_bin_/g' | sed 's/_k95/\t/g' | sed 's/_NODE/\t/g' | sed 's/_scaffold/\t/g' | grep "vRhyme" | cut -f1,2,3 | awk '{ print $1 " " $3 "_" $2}' > mcl_vMAGs_renamed.txt
```
**Used this to create mcl_formatted_table_VentPlumeViruses_renamedFinal, which is the final mcl cluster file with the right names for all 1kb viruses from Plume and Vent**

### Redoing mcl clustering of 1kb PlumeVent skani output for higher aligned fraction

1. To only get output for viral genomes where aligned fraction is >50%
```{bash, eval = FALSE}
awk '{ if ($4 >= 50 && $5 >= 50) print $1 "\t" $2 "\t" $3 "\t" $4 "\t" $5 "\t" $6 "\t" $7 }' skani_ANI_vUnbinned_vMAGs_PlumeVents.tsv > skani_ANI_vUnbinned_vMAGs_PlumeVents_50AF.tsv
```

2. Preprocess that file - I had to do this on sulfur because needed polars from mcl conda environment. Remember has to be older version if you're getting the error about sep vs separation
```{bash, eval = FALSE}
/storage1/data14/software/skani-vMAG/python/preprocess.py -i skani_ANI_vUnbinned_vMAGs_PlumeVents_50AF.tsv -o skani_ANI_vUnbinned_vMAGs_PlumeVents_50AF_processed.tsv
```
**Notice that lowest value from this processed file is much higher than the lowest from the old processed file**

3. Run mcxload
```{bash, eval = FALSE}
mcxload -abc skani_ANI_vUnbinned_vMAGs_PlumeVents_50AF_processed.tsv -o mcl/vUnbinned_vMAGs_PlumeVents_50AF.mci -write-tab mcl/vUnbinned_vMAGs_PlumeVents_50AF.mcxload
```

4. Run mcl
```{bash, eval = FALSE}
mcl vUnbinned_vMAGs_PlumeVents_50AF.mci -use-tab vUnbinned_vMAGs_PlumeVents_50AF.mcxload -o out_vUnbinned_vMAGs_PlumeVents_50AF.clusters
```
1,021 clusters found

5. Convert to csv
```{bash, eval = FALSE}
sed 's/\t/,/g' out_vUnbinned_vMAGs_PlumeVents_50AF.clusters > out_vUnbinned_vMAGs_PlumeVents_50AF.clusters.csv
```

## Final clustering with skani and mcl, Vent and Plume, 5kb and above viruses

1. Set up directories of viral genomes by creating sym links for vMAGs >=5kb. Also get 1 file of unbinned viruses >5kb. Remember to replace "=" in name to "_" because of issues.
```{bash, eval = FALSE}
pwd
/storage1/data12/Projects/Vent_Viruses/mcl_clusters/PlumeVent_5kb

sed 's/$/.fasta/g' 5kb_AllViruses_list.txt > 5kb_PlumeVentViruses_list_noEqual.txt
sed -i 's/=/_/g' 5kb_PlumeVentViruses_list_noEqual.txt
grep "vRhyme" 5kb_PlumeVentViruses_list_noEqual.txt > 5kb_PlumeVentViruses_list_vMAGsOnly.txt

for i in `cat 5kb_PlumeVentViruses_list_vMAGsOnly.txt`; do ln -s /storage1/data12/Projects/Vent_Viruses/dRep/input_viruses_fna/$i fna_vMAGs/; done

perl screen_list_new.pl 5kb_vUnbinned.txt unbinned_PlumeVentViruses.fna keep > vUnbinned_PlumeVent_Viruses_5kb.fna
```

2. Rerun ANI with skani
```{bash, eval = FALSE}
conda activate

nohup /storage1/data14/software/skani-vMAG/python/skani-vMAG.py -c vUnbinned_PlumeVent_Viruses_5kb.fna -d /storage1/data12/Projects/Vent_Viruses/mcl_clusters/PlumeVent_5kb/fna_vMAGs -x .fasta --outdir . &

cp skani_ANI.tsv skani_ANI_PlumeVent_Viruses_5kb.tsv
```

3. Preprocess the skani ANI results to cluster with mcl
```{bash, eval = FALSE}
conda activate mcl

/storage1/data14/software/skani-vMAG/python/preprocess.py -i skani_ANI_PlumeVent_Viruses_5kb.tsv -o skani_ANIprocessed_PlumeVent_Viruses_5kb.tsv
```

4. Run clustering with mcl
```{bash, eval = FALSE}
conda activate mcl

mcxload -abc skani_ANIprocessed_PlumeVent_Viruses_5kb.tsv -o PlumeVent_Viruses_5kb.mci -write-tab PlumeVent_Viruses_5kb.mcxload

nohup mcl PlumeVent_Viruses_5kb.mci -use-tab PlumeVent_Viruses_5kb.mcxload -o PlumeVent_Viruses_5kb.clusters &

sed 's/\t/,/g' PlumeVent_Viruses_5kb.clusters > PlumeVent_Viruses_5kb.clusters.csv
```

## Redoing skani with only sulfur cycling viruses

1. Rerun skani on just sulfur cycling viruses (on Sulfur)
```{bash, eval = FALSE}
nohup /storage1/data14/software/skani-vMAG/python/skani-vMAG.py -c sulfur_vUnbinned.fna -d VirusGenomes/vMAGs/ -x .fasta --outdir . &

mv skani_ANI.tsv skani_ANI_VentPlume_sulfurViruses.tsv

/storage1/data14/software/skani-vMAG/python/preprocess.py -i skani_ANI_VentPlume_sulfurViruses.tsv -o skani_ANI_VentPlume_sulfurViruses_processed.tsv
```

2. Filter for >50AF
```{bash, eval = FALSE}
awk '{ if ($4 >= 50 && $5 >= 50) print $1 "\t" $2 "\t" $3 "\t" $4 "\t" $5 "\t" $6 "\t" $7 }' skani_ANI_VentPlume_sulfurViruses.tsv > skani_ANI_VentPlume_sulfurViruses_50AF.tsv
```

3. Also preprocess the sulfur only viruses ANI with AF50
```{bash, eval = FALSE}
/storage1/data14/software/skani-vMAG/python/preprocess.py -i skani_ANI_VentPlume_sulfurViruses_50AF.tsv -o skani_ANI_VentPlume_sulfurViruses_50AF_processed.tsv
```
**This made me realize the problem with skani triangle is it won't filter by AF. With this method I just used, I can see the ANI of sulfur viruses for those that have AF ≥50%. Do this, get the viruses, then use those in skani triangle for more accurate picture of ANI**

4. Rerun skani triangle to get the matrix for ANI plotting using new info learned. Here I am only giving it the viruses I know have an aligned fraction of ≥50 for **both** viruses, not just 1 way. You can use the --min_af option but might still get misleading results because one of the virus pairs could have low % aligned fraction.
```{bash, eval = FALSE}
skani triangle -s 80 --full-matrix --min-af 49 sulfur_genomes_for_SkaniTriangle/* > skani_ani_sulfur_FullMatrix.txt

sed 's+sulfur_genomes_for_SkaniTriangle/++g' skani_ani_sulfur_FullMatrix.txt > skani_ani_sulfur_FullMatrix_renamed.txt
```

## Get ANI per cluster using Cody's git repo

1. Install
```{bash, eval = FALSE}
git clone https://github.com/cody-mar10/skani-vMAG.git
cd skani-vMAG
mamba create -n vskani -c conda-forge -c bioconda "skani>=0.1" "polars>=0.18"
pip install -e .
```

2. Run
```{bash, eval = FALSE}
vskani summarize -i skani/PlumeVent/skani_ANI_vUnbinned_vMAGs_PlumeVents.tsv -mo mcl/PlumeVent/vUnbinned_vMAGs_PlumeVents.clusters -o ani_skani_PlumeVent.tsv
```

```{bash, eval = FALSE}
vskani summarize -i skani_ANI_vUnbinned_vMAGs_PlumeVents_50AF.tsv -mo mcl/out_vUnbinned_vMAGs_PlumeVents_50AF.clusters -o ani_skani_PlumeVent_50AF.tsv
```

### Rerun skani and mcl using Cody's git repo and dif parameters

1. Running on virserver
```{bash, eval = FALSE}
nohup vskani all -c Virus_Genomes/fna/unbinned_PlumeVentViruses.fna -d Virus_Genomes/fna/fna_vMAGs_separate/ -x .fasta --outdir skani/PlumeVent_500m30cm -o skani_ANI_VentPlume_500m30cm.tsv -m500 -cm 30 -s 70 -f 50 -t 30 -ma .7  --outdir . &
```

2. Didn't make it to mcl so running that manually:
```{bash, eval = FALSE}
mcxload -abc skani_processed.tsv -o mcl/vUnbinned_vMAGs_PlumeVents.mci -write-tab mcl/vUnbinned_vMAGs_PlumeVents.mcxload

mcl vUnbinned_vMAGs_PlumeVents.mci -use-tab vUnbinned_vMAGs_PlumeVents.mcxload -o mcl_VentPlume_skani500m30cm.clusters
```

3. Summarize ANI values
```{bash, eval = FALSE}
vskani summarize -i ../skani_ANI_VentPlume_500m30cm.tsv -mo mcl_VentPlume_skani500m30cm.clusters -o ani_skani_500m30cm_PlumeVent.tsv
```

### Same as above but with ≥3kb viruses

I was being dumb before, just needed to install mcl in the conda env. Then it is good to go and produces all output.

```{bash, eval = FALSE}
nohup vskani all -c unbinned_PlumeVentViruses_3kb.fna -d 3kb_vMAGs/ -x .fasta -o skani_ANI_VentPlume_500m30cm_3kb.tsv -m500 -cm 30 -s 70 -f 50 -t 30 -ma .7  --outdir . &
```

## Clustering using dRep

1. Split the unbinned viruses into individual files. I realized I need to replace the "=" in the header name because Unix does not like it
```{bash, eval = FALSE}
splitfasta unbinned_VentViruses.fasta

for i in $(ls)
do
  name1=$(cat "$i" | grep \> | sed 's/$/.fasta/g' | sed 's/>//g')
  echo mv "$i" "${name1}"
done

./rename_script.sh
```

2. Symlink the individual virus genomes. Has to be done in a for loop because normal symlink method is too many arguments when bash is searching for the paths.
```{bash, eval = FALSE}
for i in /storage1/data12/Projects/Vent_Viruses/Virus_Genomes/fna/vUnbinned_fna/unbinned_VentViruses_split_files/*fasta; do ln -s $i .; done
```

3. Repeat for the Plume unbinned viruses. I think the "=" is a problem in the names so I am changing it to underscore for the purposes of dRep
```{bash, eval = FALSE}
sed 's/=/_/g' unbinned_PlumeViruses.fna > unbinned_PlumeViruses_renamed.fasta

splitfasta unbinned_PlumeViruses_renamed.fasta

for i in $(ls)
do
  name1=$(cat "$i" | grep \> | sed 's/$/.fasta/g' | sed 's/>//g')
  echo mv "$i" "${name1}"
done

./rename_script.sh

ln -s /storage1/data12/Projects/Plume_Viruses/Virus_Genomes/unbinned_PlumeViruses_renamed_split_files/*fasta .
```

To confirm have the expected total of viruses:
```{bash, eval = FALSE}
ls | wc -l
```
**38,014 viruses total**

4. **Nevermind, this step isn't necessary because you can ignore Genome qual**. Compile the CheckV info to provide to dRep: genome,completeness,contamination

```{bash, eval = FALSE}
cat CheckV_Output/quality_summary.tsv CheckV_vMAGs/quality_summary_vMAGs_Vents.tsv > checkV_vUnbinned_vMAGs_Vents.tsv
```

5. Run dRep dereplicate - using the guidance provided at the bottom of [this page](https://drep.readthedocs.io/en/latest/choosing_parameters.html) for clustering bacteriophages and plasmids

First, make a txt file of the paths to the vMAGs because list is too long to call with a wildcard.
```{bash, eval = FALSE}
ls > ../vMAG_paths.txt
sed -i 's+^+/storage1/data12/Projects/Vent_Viruses/dRep/input_viruses_fna/+g' vMAG_paths.txt
```

```{bash, eval = FALSE}
nohup dRep dereplicate /storage1/data12/Projects/Vent_Viruses/dRep -g vMAG_paths.txt --S_algorithm ANImf -nc .5 -l 1000 -N50W 0 -sizeW 1 --ignoreGenomeQuality --clusterAlg single -p 15 &
```
**This run with ANImf (uses nucmer to align genomes) is estimated to take ~1 week with >30k viral genomes. So I am cancelling the run and switching to fastANI. I'll probably lose some accuracy but I guess it's necessary because a week per run won't allow me to tweak parameters if necessary.**

Final command used was this:
```{bash, eval = FALSE}
/storage1/data12/miniconda3/envs/drep/bin/dRep dereplicate /storage1/data12/Projects/Vent_Viruses/dRep -g vMAG_paths.txt --S_algorithm fastANI -nc .5 -l 1000 -N50W 0 -sizeW 1 --ignoreGenomeQuality --clusterAlg single -p 15
```

6. Analyze dRep output. Use the following command to see how many representatives are in each cluster:
```{bash, eval = FALSE}
cut -d "," -f6 Cdb.csv | sort | uniq -c | sort -nr
```
14,942 clusters total.

Get the cluster ID of every cluster that has 2 or more representatives
```{bash, eval = FALSE}
cut -d "," -f6 Cdb.csv | sort | uniq -c | sort -nr | head -n1657 > ../cdb_clusterIDs_2orMore.txt
```
**There are 1,657 clusters that have 2 or more representatives. 13,285 are singletons.** 

Loop to get all the names of all the reps in a cluster
```{bash, eval = FALSE}
for i in `cat clusterIDs_toGrab.txt`; do grep -w $i Cdb.csv | cut -d "," -f1 >> $i.ViralGenomes.txt; done
```

Now add the file name to each txt file so can cat them all together
```{bash, eval = FALSE}
awk -i inplace -v ORS='\r\n' 'FNR==1{print FILENAME}1' *
```

## dRep with 5kb viruses
1. There should be 7297 viruses with genomes >5kb. Put them into 1 folder for dRep. Then make the virus genome path file
```{bash, eval = FALSE}
for i in `cat 5kb_PlumeVentViruses_list_noEqual.txt`; do cp /storage1/data12/Projects/Vent_Viruses/dRep/input_viruses_fna/$i input_VentPlume_viruses_fna_5kb/; done

ls *fasta > ../5kb_VirusGenomes_paths.txt

sed -i 's+^+/storage1/data12/Projects/Vent_Viruses/dRep/input_viruses_fna/+g' 5kb_VirusGenomes_paths.txt

sed -i 's+/storage1/data12/Projects/Vent_Viruses/dRep/input_viruses_fna/+/storage1/data12/Projects/Vent_Viruses/dRep/dRep_5kb/input_VentPlume_viruses_fna_5kb/+g' 5kb_VirusGenomes_paths.txt

```

2. Run it
```{bash, eval = FALSE}
conda activate drep

nohup /storage1/data12/miniconda3/envs/drep/bin/dRep dereplicate /storage1/data12/Projects/Vent_Viruses/dRep/dRep_5kb -g 5kb_VirusGenomes_paths.txt --S_algorithm fastANI -nc .5 -l 1000 -N50W 0 -sizeW 1 --ignoreGenomeQuality --clusterAlg single -p 15 &
```

## dRep with 5kb viruses at 90% ANI

```{bash, eval = FALSE}
conda activate drep

nohup /storage1/data12/miniconda3/envs/drep/bin/dRep dereplicate /storage1/data12/Projects/Vent_Viruses/dRep/dRep_5kb_90ani -g 5kb_VirusGenomes_paths.txt --S_algorithm fastANI -sa 0.90 -nc .5 -l 1000 -N50W 0 -sizeW 1 --ignoreGenomeQuality --clusterAlg single -p 20 --skip_plots &
```

## dRep with 1kb viruses at 90% ANI

```{bash, eval = FALSE}
nohup /storage1/data12/miniconda3/envs/drep/bin/dRep dereplicate /storage1/data12/Projects/Vent_Viruses/dRep/dRep_1kb_90ani/ -g 1kb_VirusGenomes_paths.txt --S_algorithm fastANI -sa 0.90 -nc .5 -l 1000 -N50W 0 -sizeW 1 --ignoreGenomeQuality --clusterAlg single -p 20 --skip_plots &
```

## Realized dRep has been throwing errors for some clusters

Looks like on the 5kb run, ~190 viral scaffolds are not making it into the final clustering so trying to troubleshoot errors here:
```{bash, eval = FALSE}
conda activate drep

sed 's+/dRep/input_viruses_fna+/dRep/dRep_1kb/input_viruses_fna+g' vMAG_paths.txt > ../1kb_VirusGenomes_paths.txt

nohup /storage1/data12/miniconda3/envs/drep/bin/dRep dereplicate /storage1/data12/Projects/Vent_Viruses/dRep/dRep_5kb_debug -g 5kb_VirusGenomes_paths.txt --S_algorithm fastANI -nc .5 -l 5000 -N50W 0 -sizeW 1 --ignoreGenomeQuality --clusterAlg single -p 15 --debug --skip_plots &
```



## skani v0.2.0 

Rerunning skani and whole pipeline with skani version0.2.0 because of major bug with previous version where ANI calculations were off for >5k genomes.

1. vskani not functioning as expected yet so I am running skani, filtering it for 50AF, then using those genomes to cluster with mcl

```{bash, eval = FALSE}
mamba activate vskani
mamba remove skani
source ~/.bashrc

nohup vskani all -c unbinned_PlumeVentViruses_3kb.fna -d 3kb_vMAGs/ -x .fasta -o skani2_ANI_VentPlume_200m30cm_3kb.tsv -m 200 -cm 30 -s 70 -f 50 -t 30 -ma .7  --outdir . &

awk '{ if ($4 >= 50 && $5 >= 50) print $1 "\t" $2 "\t" $3 "\t" $4 "\t" $5 "\t" $6 "\t" $7 }' skani2_ANI_VentPlume_200m30cm_3kb.tsv > skani2_ANI_VentPlume_200m30cm_3kb_50AF.tsv
```

2. Then downloaded the 50AF file to remove self matches so could get an accurate list of virus names for clustering
```{bash, eval = FALSE}
cut -f1 skani2_ANI_VentPlume_noSelf.tsv | grep -v "unbinned" | sort | uniq > vMAGs.txt

cat vUnbinned.txt vUnbinned_2.txt | sort | uniq > vUnbinned_final.txt

for i in `cat vMAGs.txt`; do cp ../../PlumeVent_500m30cm_3kb/3kb_vMAGs/$i 3kb_vMAGs_50AF/; done

perl /scratch/langwig/scripts/screen_list_new.pl vUnbinned_final.txt ../../PlumeVent_500m30cm_3kb/unbinned_PlumeVentViruses_3kb.fna keep > unbinned_PlumeVentViruses_3kb_50AF.fna
```
Col 1 gives same name results as col 2. 6 and 7 had unique unbinned.

3. run vskani on 50AF genomes
```{bash, eval = FALSE}
nohup vskani all -c unbinned_PlumeVentViruses_3kb_50AF.fna -d vMAGs_3kb_50AF/ -x .fasta -o skani2_ANI_VentPlume_200m30cm_3kb_50AF.tsv -m 200 -cm 30 -s 70 -f 50 -t 30 -ma .7  --outdir . &
```
Then import the skani tsv file into R for processing.


I realized I want to filter the skani results before clustering. So I took the file `skani2_ANI_VentPlume_200m30cm_3kb.tsv` produced above from `vskani all` and created the processed file in R with 3 columns, no headers, calling it `skani2_ANI_VentPlume_200m30cm_3kb_preprocessed.tsv`. ANI was normalized by lowest AF (ANI*lowest AF/100^2) and filtered for ≥70. Now finishing clustering manually:

```{bash, eval = FALSE}
conda activate vskani

mcxload -abc skani2_ANI_VentPlume_200m30cm_3kb_preprocessed.tsv -o skani_processed.mci -write-tab skani_processed.mcxload

nohup mcl skani_processed.mci -use-tab skani_processed.mcxload -o dereplicated_virus.clusters &
```

# Using Seqkit to get genome size of vMAGs

1. I have to run this because the CheckV contig length is going to be off since that was run on vMAGs that were N-linked.
```{bash, eval = FALSE}
conda activate seqkit

mkdir Seqkit

nohup sh -c 'for i in *.fasta ; do seqkit stats $i >> Seqkit/vMAG_Seqkit_stats.tsv ; done' &

less vMAG_Seqkit_stats.tsv  | sed '/^file/d'  | sed 's/.fasta//g' | sed 's/\s\s*/\t/g' > vMAG_stats_parsed.tsv

cut -f1,4-8 vMAG_stats_parsed.tsv | sed '1i Genome\tnum_seqs\tsum_len\tmin_len\tavg_len\tmax_len' > vMAG_stats_parsed_calc.tsv
```
Doing the above and getting the calc file is all you need, I did the part below thinking I need GB...but KB is fine.

```{bash, eval = FALSE}
sed 's/\,//g' vMAG_stats_parsed_calc.tsv | tail +2 | awk '{ printf "%.3f\n", $3/1e+06 }' | sed '1i Genome_Size_MB' > result.txt

paste vMAG_stats_parsed_calc.tsv result.txt > vMAG_GenSizeMB.tsv
```

2. Download the file and combine vMAG genome size in KB with the length of the unbinned virus from column 2 of the CheckV output. Now you have the file of all viruses, binned and unbinned, with genome size in KB to use for plotting and quality checking results.
```{bash, eval = FALSE}
cut -f1,2 quality_summary_vUnbinned_Vents.tsv > vUnbinned_GenSize_KB.tsv

cat Vent_vMAG_GenSize_KB.tsv vUnbinned_GenSize_KB.tsv > Vent_vUnbinned_vMAG_GenSize_KB.tsv
```

Rerun Seqkit genome size on all viruses vent and plume, vMAG and vUnbinned, all at once. (Reminder CheckV contig length is going to be off since that was run on vMAGs that were N-linked). Doing this on Virserver, 12/12/23. Seqkit Version 2.6.1
```{bash, eval = FALSE}
mamba activate

mkdir seqkit

nohup sh -c 'for i in *.fasta ; do seqkit stats $i >> Seqkit/vMAG_Seqkit_stats.tsv ; done' &

cat VentPlume_seqkit_stats.tsv | sort | uniq > VentPlume_seqkit_stats_uniq.tsv

less VentPlume_seqkit_stats.tsv  | sort | uniq | 's+../Virus_Genomes/fna/all_fna_vMAG_vUnbinned/++g' | sed 's/.fasta//g' > VentPlume_seqkit_stats_renamed.tsv

less VentPlume_seqkit_stats_renamed.txt | tr -s ' ' > VentPlume_seqkit_stats_renamed_space.txt

sed -i 's/\,//g' VentPlume_seqkit_stats_renamed_space.txt
```


# Run GTDBtk v1.5.0 on Vent MAGs

```{bash, eval = FALSE}
conda activate gtdbtk-1.5.0

nohup gtdbtk classify_wf --genome_dir MAGs_Renamed/No_ViralContam/ --out_dir GTDBtk_v1.5.0/ --cpus 15 &
```

# Using Cody's script to parse dRep and skani output

**You need python3.11 for this script**

1. dRep parsing
-a points to the ANI file, which is Nbd.csv for dRep. -c to clusters file, Cdb.csv for dRep
```{bash, eval = FALSE}
conda create -n py311 python=3.11
conda activate py311

mamba install -c conda-forge polars networkx matplotlib numpy python=3.11

/storage1/data14/for_maggie/calc_ani_per_cluster.py -a data_tables/Ndb.csv -c data_tables/Cdb.csv -m fastani-drep
```

2. skani parsing
```{bash, eval = FALSE}
/storage1/data14/for_maggie/calc_ani_per_cluster.py -a /storage1/data12/Projects/Vent_Viruses/skani/PlumeVent/skani_ANI_vUnbinned_vMAGs_PlumeVents.tsv -c /storage1/data12/Projects/Vent_Viruses/mcl_clusters/PlumeVent/vUnbinned_vMAGs_PlumeVents.clusters -m skani-mcl
```

# Using Codys rfasta to split faa unbinned viruses

```{bash, eval = FALSE}
rfasta split -i All_Viruses.faa -d vUnbinned_faa_split -m genome
```

# Clustering viral protein content using mmseqs

1. Run mmseqs. 595,541 proteins between Plume and Vent binned and unbinned viruses.
```{bash, eval = FALSE}
conda install -c bioconda mmseqs2

mmseqs createdb PlumeVent_Viruses.faa PlumeVent_Viruses_DB

nohup mmseqs cluster PlumeVent_Viruses_DB PlumeVent_Viruses_DB_clu /storage1/data12/tmp --cov-mode 0 --min-seq-id 0.75 --threads 20 &

mmseqs createtsv PlumeVent_Viruses_DB PlumeVent_Viruses_DB PlumeVent_Viruses_DB_clu PlumeVent_mmseqs_clusters.tsv --threads 20
```
cov-mode 0 means alignment covers [at least 0.8 of query and of target](https://github.com/soedinglab/mmseqs2/wiki#how-to-set-the-right-alignment-coverage-to-cluster). Using default clustering mode, --cluster-mode 0. This is the [greedy set cover algorithm](https://github.com/soedinglab/mmseqs2/wiki#clustering-modes), which iteratively selects the node with most connections and all its connected nodes to form a cluster and repeats until all nodes are in a cluster.\n

[See here](https://github.com/soedinglab/mmseqs2/wiki#cluster-tsv-format) for explanation of tsv output format.

# Genomad for virus taxonomy

1. Add this to your .bashrc first
```{bash, eval = FALSE}
export LANG="en_US.UTF-8"
```

2. Download genomad with mamba and run it. **Caution when running** genomad uses tensorflow during the classification step and at this step it will significantly overuse the amount of threads you set. It seems like it's using all available CPUs?
```{bash, eval = FALSE}
mamba create -n genomad -c conda-forge -c bioconda genomad

mamba activate genomad

nohup genomad end-to-end --cleanup Virus_Genomes/vUnbinned_vMAGs_1500Ns_PlumeVent.fna genomad_output /storage2/databases/genomad_db/ -t 30 &
```

3. Use Cody's parsing script to get the taxonomy info from genomad into GTDBtk format
```{bash, eval = FALSE}
conda activate

conda install -c conda-forge polars

python /storage1/data14/for_maggie/genomad/fix_genomad_taxonomy.py -i vUnbinned_vMAGs_1500Ns_PlumeVent_taxonomy.tsv -o vUnbinned_vMAGs_1500Ns_PlumeVent_genomad_tax_parsed.txt
```


# Focus on viruses that infect sulfur cycling microbes

## Annotating microbial MAGs

1. Copy data to virserver. Cpying MAGs with no viral contamination. Renamed to remove NoViralContam from names.
```{bash, eval = FALSE}
cp /storage1/data12/Projects/Vent_Viruses/MAGs_Renamed/No_ViralContam/*fna .
```

2. Plume MAGs I already have the faa format MAGs without viral contamination. So just going to translate Vent MAGs on sulfur and then cp faa files to virserver.
```{bash, eval = FALSE}
for sample in *.fna; do prodigal -i $sample -a Prodigal/$sample.faa -o Prodigal/$sample_output.txt; done
```

3 Run the hmm search
```{bash, eval = FALSE}
for file in /scratch/langwig/Databases/Metabolic_genes_hmms/*.hmm; do hmmsearch --cut_tc --cpu 30 --tblout $file.txt $file ../PlumeVent_MAGs_3872.faa; echo "next hmm"; done
```

4. Get best hits
```{bash, eval = FALSE}
for i in *.txt; do python3 /storage2/scratch/langwig/scripts/hmm_parser.py -i $i -o $i.parsed.txt; done
```
**Note that some are 0 because did not have trusted cutoff**. \n
**Also this did not actually grab best hits as expected but organized nicely**

## Annotating MAGs using Disco

```{bash, eval = FALSE}
nohup perl /storage2/scratch/langwig/Tools/DiSCo/tool/DiSCo.pl -i MAGs_microbial/PlumeVent_MAGs_3872.faa -d /storage2/scratch/langwig/Projects/VentPlumeViruses/Disco/ -o -n 30 -m /storage2/scratch/langwig/Tools/DiSCo/tool/DiSCo_HMMlib -f /storage2/scratch/langwig/Tools/DiSCo/tool/filter_DiSCo.pl &
```


### Sulfur hmm search parsing

**File that is named subset_sulfurHMM_MAG_names_final_uniq.txt contains the names of microbial MAGs that encode aprA, dsrABD, fccB, soxBCY, or phsA**

Wild sed commands to get rid of annoying names...
```{bash, eval = FALSE}
sed 's/_355-202/\t/2' subset_sulfurHMM_MAGs_final_uniq.txt | cut -f1 | sed 's/_354-166/\t/2' | sed 's/_4571-419/\t/2' | sed 's/_4562-384/\t/2' | sed 's/_4562-384/\t/2' | sed 's/_4561-380/\t/2' | sed 's/_4559-240/\t/2' | sed 's/_4281-140/\t/2' | sed 's/_4562-384/\t/2' | sed 's/_PIR-30/\t/2' | sed 's/_V2/\t/2' | sed 's/_T2/\t/2' | sed 's/_T11/\t/2' | sed 's/_T10/\t/2' | sed 's/_M2/\t/2' | sed 's/_M17/\t/2' | sed 's/_M1/\t/2' | cut -f1 | sed 's/_A3/\t/2' | sed 's/_A1/\t/2' | sed 's/_134-614/\t/2' | sed 's/_132-544/\t/2' | sed 's/_131-447/\t/2' | sed 's/_128-326/\t/2' | sed 's/_S147/\t/2' | sed 's/_S146/\t/2' | sed 's/_S145/\t/2' | sed 's/_S144/\t/2' | sed 's/_S14/\t/2'| cut -f1 | sed 's/_S13/\t/2' | cut -f1 | sed 's/_S0/\t/2' | cut -f1 | sort | uniq > subset_sulfurHMM_MAGs_final_uniq_namesFixed.txt
```

## Annotations of blast-based iphop matches

What are the genes that link viruses and hosts based on blast output of iphop?

1. Get viruses that infect sulfur cyclers from BLAST file
```{bash, eval = FALSE}
for i in `cat sulfur_Virus_list.txt`; do grep -w $i ../../iphop/iphop_vUnbinned_vMAG_1500Ns_MAGdb/blastgenomes_seqids.txt >> blastgenomes_seqids_sulfur.txt; done
```

2. Then grab the matches to sulfur cycling MAGs, leave out ref genomes
```{bash, eval = FALSE}
for i in `cat sulfur_MAG_list_50comp.txt`; do grep -w $i blastgenomes_seqids_sulfur.txt >> blastgenomes_seqids_sulfur_VirusMAG.txt; done
```

3. Get coordinates for MAGs
```{bash, eval = FALSE}
cut -f3,6,7 blastgenomes_seqids_sulfur_VirusMAG.txt > MAG_coordinates.txt

bedtools getfasta -fi ../../../MAGs_microbial/PlumeVent_MAGs_3872.fna -bed MAG_coordinates.txt -fo PlumeVent_MAGs_sulfurVirus_blastGenes.fna -s
```

### Trying blastp because parsing is hard

```{bash, eval = FALSE}
diamond makedb --in sulfur_MAGs.faa -d sulfur_MAGs.db

diamond blastp --db GenomesToBlast/sulfur_MAGs.db.dmnd --query GenomesToBlast/sulfur_vUnbinned_vMAGs.faa --out VentPlume_Virus_toMAGs_diamond.txt --id 80 --evalue 1e-3 --threads 25 --outfmt 6 qtitle stitle pident length qstart qend sstart send evalue bitscore --max-target-seqs 1 &
```

## Comparison of sulfur viruses to GOV

I want to see if the viruses that infect sulfur cycling microbes are similar to viruses from the GOV.

1. Download GOV from this [link](https://de.cyverse.org/data/ds/iplant/home/shared/iVirus/GOV2.0) in CyVerse

2. Use skani to compare ANI
```{bash, eval = FALSE}
cat GOV2_viral_populations_larger_than_5KB_or_circular.fasta ../skani/sulfur_viruses/sulfur_vUnbinned.fna > sulfur_vUnbinned_GOV5kb_circ.fasta

nohup /storage1/data14/software/skani-vMAG/python/skani-vMAG.py -c sulfur_vUnbinned_GOV5kb_circ.fasta -d ../skani/sulfur_viruses/VirusGenomes/vMAGs/VirusGenomes/vMAGs/ -x .fasta --outdir . -o sulfurVirus50comp_GOV_skani_ani.tsv -f 50.0 -t 30 &
```

# Run Virsorter on viruses

YOU NEED TO MAKE SURE YOUR PYTHON IS SET UP CORRECTLY. Even though I had the correct versions in my conda env, it was still reading from /home/langwig/.local/lib and using the wrong python version, so float was deprecated. The following fixed it: 
```{bash, eval = FALSE}
rm -r /home/langwig/.local/lib

mamba create -n virsorter -c conda-forge -c bioconda "numpy<1.20" virsorter=2

nohup virsorter run -i test.fa --min-length 1500 -j 4 all -d /storage2/databases/VirSorter2/db &
```

You can use this command **inside the activated conda env** to see what python versions the env is actually using and reading.
```{bash, eval = FALSE}
python -c 'import sys; print(sys.path)'
```

```{bash, eval = FALSE}
nohup virsorter run -i Virus_Genomes/fna/unbinned_vMAGs_1500Ns_PlumeVentViruses.fna -w Virsorter/output -d /storage2/databases/VirSorter2/db/ -j 30 --keep-original-seq  --prep-for-dramv --viral-gene-enrich-off --rm-tmpdir &
```


# Comparing regions of overlap of viruses in ANI clusters

1. Set up so that the viral genomes in a cluster are in separate folders.

a. Split virus genome file into individual files (had to rename = to _)
```{bash, eval = FALSE}
rfasta split -i unbinned_viruses_GeoDistinct.fna -d unbinned_fna_split -m genome
```

b. split txt file into many txt files
```{bash, eval = FALSE}
pwd
/scratch/langwig/Projects/VentPlumeViruses/skani/virus_alignments/txt_files/split_txts

awk -F'\t' '{print>$2".txt"}' viruses_GeoDistinct.txt

for i in *txt; do cut -f1 $i >> $i.renamed; done
for i in *renamed; do sed 's/$/.fasta/g' $i >> $i.renamed; done
rename 's/txt.renamed.renamed/_final.txt/g' *renamed.renamed
rename 's/._final/_final/g' *final.txt
for i in *final.txt; do sed -i 's/=/_/g' $i; done
```

c. Created copy_files.sh with the help of chatGPT. Script is the following to use txt files to make fna files of all genomes per cluster:
```{bash, eval = FALSE}
#!/bin/bash

# Directory where the text files are located
source_txt_dir="/storage2/scratch/langwig/Projects/VentPlumeViruses/skani/virus_alignments/txt_files/split_txts"
# Directory where the files to be concatenated are located
source_files_dir="/storage2/scratch/langwig/Projects/VentPlumeViruses/skani/virus_alignments/fna_genomes"
# Directory where you want to save the concatenated files with the "fna" extension
destination_dir="/storage2/scratch/langwig/Projects/VentPlumeViruses/skani/virus_alignments/blastn_ClustViruses"

# Loop through each text file
for txt_file in "$source_txt_dir"/*.txt; do
    if [ -f "$txt_file" ]; then
        # Get the directory name without the ".txt" extension
        directory_name=$(basename "$txt_file" .txt)

        # Create an output file with the same name as the text file (without ".txt") and ".fna" extension
        output_file="$destination_dir/$directory_name.fna"

        # Initialize an empty concatenated file
        > "$output_file"

        # Read each filename from the text file and concatenate the corresponding files
        while IFS= read -r filename; do
            cat "$source_files_dir/$filename" >> "$output_file"
        done < "$txt_file"
    fi
done
```

2. Run blastn on viruses

a. Get fnas into separate directories for running blastn
```{bash, eval = FALSE}
rename 's/_final/_VirusClust/g' *fna

#!/bin/bash

# Directory where the FNA files are located
fna_dir="/storage2/scratch/langwig/Projects/VentPlumeViruses/skani/virus_alignments/blastn_ClustViruses"

# Loop through each FNA file
for fna_file in "$fna_dir"/*.fna; do
    if [ -f "$fna_file" ]; then
        # Get the base name of the file (without extension)
        file_name=$(basename -- "$fna_file")
        directory_name="${file_name%.fna}"

        # Create the directory with the same name as the FNA file (without "fna")
        mkdir -p "$directory_name"

        # Copy the FNA file into the corresponding directory
        cp "$fna_file" "$directory_name/"
    fi
done
```

b. Run makeblastdb on all the fnas. Used version 2.14.1+ of makeblastdb.
```{bash, eval = FALSE}
#!/bin/bash

# Directory where the FNA files are located
base_dir="/scratch/langwig/Projects/VentPlumeViruses/skani/virus_alignments/blastn_ClustViruses"

# Loop through directories containing FNA files with "VirusClust" in their names
for dir in "$base_dir"/*VirusClust*/; do
    if [ -d "$dir" ]; then
        # Extract the directory name without the path
        dir_name=$(basename "$dir")

        # Loop through FNA files in the current directory
        for fna_file in "$dir"/*.fna; do
            if [ -f "$fna_file" ]; then
                # Get the base name of the file (without extension)
                file_name=$(basename -- "$fna_file")
                file_name_no_ext="${file_name%.fna}"

                # Define the output database file path with "_db" suffix
                database_file="$dir/${file_name_no_ext}_db"

                # Run makeblastdb
                makeblastdb -in "$fna_file" -out "$database_file" -dbtype nucl
            fi
        done
    fi
done

```

c. Run blastn on all the fnas with their dbs.
```{bash, eval = FALSE}
./run_blastn.sh

#!/bin/bash

# Directory where the FNA files are located
base_dir="/scratch/langwig/Projects/VentPlumeViruses/skani/virus_alignments/blastn_ClustViruses"

# Loop through directories containing FNA files with "VirusClust" in their names
for dir in "$base_dir"/*VirusClust*/; do
    if [ -d "$dir" ]; then
        # Extract the directory name without the path
        dir_name=$(basename "$dir")

        # Loop through FNA files in the current directory
        for fna_file in "$dir"/*.fna; do
            if [ -f "$fna_file" ]; then
                # Get the base name of the file (without extension)
                file_name=$(basename -- "$fna_file")
                file_name_no_ext="${file_name%.fna}"

                # Define the output database file path with "_db" suffix
                database_file="$dir/${file_name_no_ext}_db"

		# Define the output file with ".txt" suffix
                output_file="$dir/${file_name_no_ext}.txt"

                # Run makeblastdb
                blastn -query "$fna_file" -db "$database_file" -out "$output_file" -outfmt "6 qseqid sseqid evalue bitscore length pident qstart qend sstart send" -max_target_seqs 2 -max_hsps 1
            fi
        done
    fi
done
```

3. Parse output to get regions of overlap among viruses.

a. I created an R script to read the blast output file and remove self to self blasts as well as duplicate comparisons when the first sequence and second sequence are compared identically but flipped order (subject vs query sequence).

```{bash, eval = FALSE}
find -name "*.txt" -exec cp {} . \;
cat *txt > all_VirusClust_blastn.txt

Rscript parse_blastn.R
```

b. Take filtered_blastn_results.txt file produced from R script and extract coordinates you want to grab.  
```{bash, eval = FALSE}
cut -f1,7,8 filtered_blastn_results.txt > query_results.txt
cut -f2,9,10 filtered_blastn_results.txt > subj_results.txt

cat query_results.txt subj_results.txt > all_VirusClust_blastn_filtered.txt
```

4. bedtools to obtain ORFs of interest that have overlap
```{bash, eval = FALSE}
mamba install -c bioconda bedtools

bedtools intersect -a bed_GeoDistinct_VirusClusts.tsv -b bed_PlumeVent_viruses.tsv -wa -wb > result.txt
```
-f 0.50 option for 50% overlap. Or -F?

# Running coverm to obtain virus abundance

1. Get paths to all the reads in one folder using symbolic links
```{bash, eval = FALSE}
pwd
/scratch/langwig/Projects/VentPlumeViruses/Reads

find /storage2/Reads/HydroPlume/ -name "*gz" | xargs -i ln -s {} .

find /storage2/Reads/HydroVents_ZhouReysenbach/ -name "*gz" | xargs -i ln -s {} .
```

2. Get paths to all viral genomes in one folder using symbolic links
```{bash, eval = FALSE}
pwd
/scratch/langwig/Projects/VentPlumeViruses/abundance/Genomes_fna

find ../../Virus_Genomes/fna/fna_vMAGs_separate/ -name "*fasta" | xargs -i ln -s {} .

./split_fasta.sh unbinned_PlumeVentViruses_renamed.fna

find ../../Virus_Genomes/fna/split_fasta_files/ -name "*fasta" | xargs -i ln -s {} .
```

3. Run the bash script to run coverm
```{bash, eval = FALSE}
time nohup ./run_coverm.sh &
```

4. Running again with the count option in --methods to also get count of reads mapped
```{bash, eval = FALSE}
mamba activate coverm

nohup ./run_coverm_count.sh &
```
Had to add --min-covered-fraction 0 to the script based on error: "The 'counts' coverage estimator cannot be used when --min-covered-fraction is > 0 as it does not calculate the covered fraction. You may wish to set the --min-covered-fraction to 0 and/or run this estimator separately."

5. Running again with the covered_fraction option in --methods to also get covered fraction because I want to filter for >75 covered fraction. Also added --min-read-percent-identity 90 --min-read-aligned-length 50 to be more stringent on reads mapped.
```{bash, eval = FALSE}
mamba activate coverm

nohup ./run_coverm_count_MinCov.sh &
```


# Translate viruses to protein yourself using prodigal
1. Translate to proteins because confusion with the files I have. On virserver
```{bash, eval = FALSE}
cat vMAGs_PlumeVent_noNs.fna unbinned_PlumeVentViruses.fna > All_PlumeVent_Viruses.fna

nohup prodigal -p meta -i All_PlumeVent_Viruses.fna -a ../faa/All_PlumeVent_Viruses_prodigal.faa -o ../faa/output.txt &
```

2. Remove E coli virus contamination
```{bash, eval = FALSE}
for i in `cat contam_virus_ecoli_list.txt; do grep $i All_PlumeVent_Viruses_prodigal.faa >> ecoli_faa_list_toRemove.txt; done

sed 's/ /\t/g' ecoli_faa_list_toRemove.txt | cut -f1 | sed 's/\(.*\)_/\1 /' | sed 's/ /\t/g' | cut -f1 | sort | uniq | wc

sed 's/>//g' ecoli_faa_list_toRemove.txt | sed 's/ /\t/g' | cut -f1 > ../faa/ecoli_faa_list_toRemove_renamed.txt

perl /scratch/langwig/scripts/screen_list_new.pl ecoli_faa_list_toRemove_renamed.txt All_PlumeVent_Viruses_prodigal.faa > All_PlumeVent_Viruses_prodigal_noEcoli.faa
```

# Run mmseqs to cluster viral proteins

Run mmseqs clustering
```{bash, eval = FALSE}
mamba install -c bioconda mmseqs2=15.6f452

mmseqs createdb All_PlumeVent_Viruses_prodigal_noEcoli.faa PlumeVent_Viruses_DB

nohup mmseqs cluster PlumeVent_Viruses_DB PlumeVent_Viruses_DB_clu /scratch/langwig/tmp --cov-mode 0 --min-seq-id 0.75 --threads 20 &

mmseqs createtsv PlumeVent_Viruses_DB PlumeVent_Viruses_DB PlumeVent_Viruses_DB_clu PlumeVent_mmseqs_clusters.tsv --threads 20
```

# Run PHROGs to get viral protein annotations

1. Run PHROGs
```{bash, eval = FALSE}
mmseqs createdb All_PlumeVent_Viruses_prodigal_noEcoli.faa PlumeVirus_mmseqs_PHROGsDB

nohup mmseqs search /storage2/databases/PHROGs/phrogs_mmseqs_db/phrogs_profile_db PlumeVirus_mmseqs_PHROGsDB results_mmseqs ./tmp --threads 20 &

mmseqs createtsv /storage2/databases/PHROGs/phrogs_mmseqs_db/phrogs_profile_db PlumeVirus_mmseqs_PHROGsDB results_mmseqs results.tsv
```

alternative command for increased sensitivity:
```{bash, eval = FALSE}
nohup mmseqs search /storage2/databases/PHROGs/phrogs_mmseqs_db/phrogs_profile_db PlumeVirus_mmseqs_PHROGsDB results_mmseqs ./tmp -s 7 --threads 20 &

nohup mmseqs search /storage1/databases/PHROGs/phrogs_mmseqs_db/phrogs_profile_db PlumeVirus_mmseqs_PHROGsDB results_mmseqs ./tmp -s 1 --threads 30 &
```

2. Filter the PHROGs output for lowest e value. Then subset the PHROGs results for the proteins identified as GD and PV to see what they are
```{bash, eval = FALSE}
Rscript parse_evalue.R

for i in `cat vir_prots_mmseqClusts_GD_PD.tsv`; do grep $i results_evalparsed.tsv >> results_GD_PD_viruses.txt; done
```


# Run dramv to get viral protein annotations

1. Run Virsorter to get dramv input file - and make sure it processes all viruses
```{bash, eval = FALSE}
nohup virsorter run --prep-for-dramv --min-score 0.0 -w /scratch/langwig/Projects/VentPlumeViruses/Virsorter -i ../Virus_Genomes/All_PlumeVent_Viruses.fna -j 100 --include-groups dsDNAphage,NCLDV,RNA,ssDNA,lavidaviridae --provirus-off --viral-gene-enrich-off all &
```

2. Run DRAMv
```{bash, eval = FALSE}
DRAM-setup.py import_config --config_loc /storage2/databases/dram-latest/CONFIG 

nohup DRAM-v.py annotate -i final-viral-combined-for-dramv_renamed.fa -v viral-affi-contigs-for-dramv_renamed.tab -o annotation --threads 50 &
```

3. Rename it for parsing
```{bash, eval = FALSE}
sed 's/__full-cat_[0-9]//g' annotations.tsv > annotations_renamed.tsv
```

4. Grab the GD and PV protein annotations
```{bash, eval = FALSE}
sed 's/__full-cat_[0-9]//g' annotations.tsv > annotations_renamed.tsv

Rscript subset_GDpd_prots.R
```

# Get total read count for all fastq reads

1. Read count using seqkit
```{bash, eval = FALSE}
nohup seqkit stats -To stats.tsv *.fastq.gz &
```





